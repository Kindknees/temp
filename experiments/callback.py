import numpy as np
import matplotlib.pyplot as plt 
import matplotlib
import io
import logging
import torch
import gymnasium as gym
from ray.rllib.algorithms.callbacks import RLlibCallback
from ray.rllib.env import BaseEnv
from ray.rllib.env.single_agent_episode import SingleAgentEpisode
from ray.rllib.env.multi_agent_episode import MultiAgentEpisode
from ray.rllib.evaluation.rollout_worker import RolloutWorker
from ray.rllib.policy import Policy
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.core.columns import Columns
from ray.tune.logger import TBXLogger
from ray.util.debug import log_once
from ray.tune.result import (TRAINING_ITERATION, TIME_TOTAL_S, TIMESTEPS_TOTAL)
from ray._private.dict import flatten_dict
# from ray.rllib.models import ModelCatalog

# from gymnasium.spaces import Discrete
from tensorboardX import SummaryWriter
from typing import TYPE_CHECKING, Dict, List, Optional, TextIO, Type, Union, Any, Tuple

if TYPE_CHECKING:
    from ray.tune.experiment.trial import Trial  # noqa: F401

logger = logging.getLogger(__name__)
VALID_SUMMARY_TYPES = [int, float, np.float32, np.float64, np.int32, np.int64] # from rllib

# Plotting settings
matplotlib.rcParams["figure.dpi"] = 200
CB91_Blue = '#2CBDFE'
CB91_Green = '#47DBCD'
CB91_Pink = '#F3A0F2'
CB91_Purple = '#9D2EC5'
CB91_Violet = '#661D98'
CB91_Amber = '#F5B14C'
color_list = [CB91_Blue, CB91_Pink, CB91_Green, CB91_Amber,
              CB91_Purple, CB91_Violet]
plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)
plt.figure(figsize=(14,10), tight_layout=True)

class LogDistributionsCallback(RLlibCallback):
    """
    Custom callback to log extra information about
    the distribution of observations and actions.

    Data saved here is later accessed by Custom Tensorboard logger.

    To use it in the experiment, add the following to the config:
    config.callbacks(LogDistributionsCallback)

    Assumes that the raw observation has the "grid" attribute that
    stores all the observations generated by the grid2op environment.
    """

    def on_episode_end(
        self,
        *,
        # Multi-agent case
        episode: Union[MultiAgentEpisode, SingleAgentEpisode],
        # Optional: Env runner and metrics
        env_runner: Optional["EnvRunner"] = None,
        metrics_logger: Optional["MetricsLogger"] = None,
        # Deprecated args
        env: Optional[gym.Env] = None,
        env_index: Optional[int] = None,
        rl_module: Optional["RLModule"] = None,
        # Legacy args
        worker: Optional[RolloutWorker] = None,
        base_env: Optional[BaseEnv] = None,
        policies: Optional[Dict[str, Policy]] = None,
        **kwargs,
    ) -> None:
        """Called when an episode is done (after terminated/truncated have been logged)."""
        
        # Handle MultiAgentEpisode
        if isinstance(episode, MultiAgentEpisode):
            # Get the info from one of the agent episodes
            if "choose_substation_agent" in episode.agent_episodes:
                agent_episode = episode.agent_episodes["choose_substation_agent"]
                # Try to get the last info
                if agent_episode.infos and len(agent_episode.infos) > 0:
                    last_info = agent_episode.infos[-1]
                    if isinstance(last_info, dict) and "steps_in_episode" in last_info:
                        episode.custom_data["num_env_steps"] = last_info["steps_in_episode"]
        
        # Handle SingleAgentEpisode
        elif isinstance(episode, SingleAgentEpisode):
            # Get the last info
            if episode.infos and len(episode.infos) > 0:
                last_info = episode.infos[-1]
                if isinstance(last_info, dict) and "steps" in last_info:
                    episode.custom_data["num_env_steps"] = last_info["steps"]
        
    def on_learn_on_batch(
        self, 
        *, 
        policy: Policy, 
        train_batch: SampleBatch,
        result: Dict,
        **kwargs
    ) -> None:
        """
        Called at the beginning of Policy.learn_on_batch().
        
        Log the action distribution and extra information about the observation.
        Note that everything in result[something] is logged by Ray.
        """
        
        # Handle both old and new column names
        obs_key = Columns.OBS if hasattr(Columns, 'OBS') else SampleBatch.OBS
        new_obs_key = Columns.NEXT_OBS if hasattr(Columns, 'NEXT_OBS') else SampleBatch.NEXT_OBS
        actions_key = Columns.ACTIONS if hasattr(Columns, 'ACTIONS') else SampleBatch.ACTIONS
        
        # Extract observations and actions
        if torch.is_tensor(train_batch[obs_key]):
            train_batch_obs = train_batch[obs_key].numpy()
            train_batch_new_obs = train_batch[new_obs_key].numpy()
            train_batch_actions = train_batch[actions_key].numpy()
        else:
            train_batch_obs = train_batch[obs_key]
            train_batch_new_obs = train_batch[new_obs_key]
            train_batch_actions = train_batch[actions_key]

        # Handle hierarchical observations
        if isinstance(train_batch_obs, dict) and "regular_obs" in train_batch_obs:
            # For hierarchical environment
            regular_obs = train_batch_obs["regular_obs"]
            regular_new_obs = train_batch_new_obs["regular_obs"]
            
            if torch.is_tensor(regular_obs):
                regular_obs = regular_obs.numpy()
                regular_new_obs = regular_new_obs.numpy()
            
            # Assuming topo_vect is the last 56 elements
            changed_topo_vec = np.any(regular_new_obs[:, -56:] != regular_obs[:, -56:], axis=-1)
            non_terminal_actions = np.any(regular_new_obs[:, -56:] != -np.ones_like(regular_new_obs[:, -56:]), axis=-1)
        else:
            # For flat environment
            changed_topo_vec = np.any(train_batch_new_obs[:, -56:] != train_batch_obs[:, -56:], axis=-1)
            non_terminal_actions = np.any(train_batch_new_obs[:, -56:] != -np.ones_like(train_batch_new_obs[:, -56:]), axis=-1)
        
        not_changed_topo_vec = 1 - changed_topo_vec
        num_non_zero_actions = np.sum(changed_topo_vec)
        
        # Log the proportion of actions that do not change the topology
        if train_batch_actions.shape[0] > 0:
            result["prop_topo_action_change"] = num_non_zero_actions / train_batch_actions.shape[0]
            
            # Calculate proportion of explicit do-nothing actions
            denom = np.sum(not_changed_topo_vec * non_terminal_actions)
            if denom > 0:
                result["prop_explicit_do_nothing"] = np.sum((train_batch_actions == 0) & (non_terminal_actions == 1)) / denom
            else:
                result["prop_explicit_do_nothing"] = 0.0
        else:
            result["prop_topo_action_change"] = 0.0
            result["prop_explicit_do_nothing"] = 0.0

        # Count all of the actions and save them to action distribution
        unique, counts = np.unique(train_batch_actions[changed_topo_vec], return_counts=True)
        action_distr_dic = {}
        
        # Initialize the dictionary with all actions
        for action in range(policy.action_space.n):
            action_distr_dic[str(action)] = 0
            
        # Fill in the actual counts
        if num_non_zero_actions > 0:
            for action, count in zip(unique, counts):
                action_distr_dic[str(action)] = count / num_non_zero_actions  # action distr in percentage
        
        result["action_distr"] = action_distr_dic
        result["num_non_zero_actions_tried"] = sum([1 for val in action_distr_dic.values() if val > 0])

    def on_postprocess_trajectory(
        self,
        *,
        worker: "EnvRunner",
        episode: Union[MultiAgentEpisode, SingleAgentEpisode],
        agent_id: Any,
        policy_id: str,
        policies: Dict[str, Policy],
        postprocessed_batch: SampleBatch,
        original_batches: Dict[Any, Tuple[Policy, SampleBatch]],
        **kwargs
    ) -> None:
        """
        Called immediately after a policy's postprocess_fn is called.
        
        Can be used for custom postprocessing of trajectory data.
        """
        # This method can be implemented if you need the opponent observation injection
        # that was commented out in the original code
        pass


class CustomTBXLogger(TBXLogger):
    """
    Custom TBX logger that logs the action distribution and extra information about the actions
    taken by the agent.

    This logger relies on the information logged by the LogDistributionsCallback.
    To use it, add to run_config: callbacks = [CustomTBXLogger]
    """

    def on_result(self, result: Dict):
        step = result.get(TIMESTEPS_TOTAL) or result[TRAINING_ITERATION]

        tmp = result.copy()
        for k in [
                "config", "pid", "timestamp", TIME_TOTAL_S, TRAINING_ITERATION
        ]:
            if k in tmp:
                del tmp[k]  # not useful to log these
        
        flat_result = flatten_dict(tmp, delimiter="/")

        # Log action distribution if available
        try:
            # Navigate through the nested structure to find action_distr
            action_distr_dic = None
            
            # Try different possible locations for the metrics in newer RLlib versions
            if "info" in result:
                if "learner" in result["info"]:
                    learner_info = result["info"]["learner"]
                    
                    # Check for default_policy
                    if "default_policy" in learner_info:
                        policy_metrics = learner_info["default_policy"]
                        if "custom_metrics" in policy_metrics and "action_distr" in policy_metrics["custom_metrics"]:
                            action_distr_dic = policy_metrics["custom_metrics"]["action_distr"]
                    
                    # Check for policy-specific keys (e.g., choose_substation_agent, choose_action_agent)
                    if action_distr_dic is None:
                        for policy_key in ["choose_substation_agent", "choose_action_agent"]:
                            if policy_key in learner_info:
                                policy_metrics = learner_info[policy_key]
                                if isinstance(policy_metrics, dict) and "custom_metrics" in policy_metrics:
                                    if "action_distr" in policy_metrics["custom_metrics"]:
                                        action_distr_dic = policy_metrics["custom_metrics"]["action_distr"]
                                        break
                    
                    # If still not found, check the first available policy
                    if action_distr_dic is None and len(learner_info) > 0:
                        for policy_key, policy_metrics in learner_info.items():
                            if isinstance(policy_metrics, dict) and "custom_metrics" in policy_metrics:
                                if "action_distr" in policy_metrics["custom_metrics"]:
                                    action_distr_dic = policy_metrics["custom_metrics"]["action_distr"]
                                    break
                
                # Also check for learner_results (newer RLlib versions)
                elif "learner_results" in result["info"]:
                    learner_results = result["info"]["learner_results"]
                    for policy_id in ["default_policy", "choose_substation_agent", "choose_action_agent"]:
                        if policy_id in learner_results:
                            if "custom_metrics" in learner_results[policy_id]:
                                if "action_distr" in learner_results[policy_id]["custom_metrics"]:
                                    action_distr_dic = learner_results[policy_id]["custom_metrics"]["action_distr"]
                                    break
            
            # If we found action distribution data, create and log the plot
            if action_distr_dic is not None:
                bar_arr = plot_to_array(bar_graph_from_dict(action_distr_dic))
                self._custom_file_writer = SummaryWriter(self.logdir, flush_secs=30)
                self._custom_file_writer.add_image("Action_distribution", bar_arr, step, dataformats="HWC")
                self._custom_file_writer.close()
                
        except Exception as e:
            if log_once("action_distr_logging_error"):
                logger.warning(f"Could not log action distribution: {e}")
        
        # Log all valid scalar and histogram values
        path = ["ray", "tune"]
        valid_result = {}
        
        for attr, value in flat_result.items():
            full_attr = "/".join(path + [attr])
            if (isinstance(value, tuple(VALID_SUMMARY_TYPES))
                    and not np.isnan(value)):
                valid_result[full_attr] = value
                self._file_writer.add_scalar(
                    full_attr, value, global_step=step)
            elif ((isinstance(value, list) and len(value) > 0)
                  or (isinstance(value, np.ndarray) and value.size > 0)):
                valid_result[full_attr] = value

                # Must be video
                if isinstance(value, np.ndarray) and value.ndim == 5:
                    self._file_writer.add_video(
                        full_attr, value, global_step=step, fps=20)
                    continue

                try:
                    self._file_writer.add_histogram(
                        full_attr, value, global_step=step)
                # In case TensorboardX still doesn't think it's a valid value
                # (e.g. `[[]]`), warn and move on.
                except (ValueError, TypeError):
                    if log_once("invalid_tbx_value"):
                        logger.warning(
                            f"You are trying to log an invalid value ({full_attr}={value}) "
                            f"via {type(self).__name__}!"
                        )
        
        self.last_result = valid_result
        self._file_writer.flush()


# Utility plotting functions
def bar_graph_from_dict(dic):
    """
    Given a dictionary, return matplotlib bar graph figure.
    """
    fig = plt.figure(figsize=(14, 10))
    ax = fig.add_subplot(111)
    
    # Sort dictionary by keys for consistent ordering
    sorted_items = sorted(dic.items(), key=lambda x: int(x[0]))
    keys, values = zip(*sorted_items) if sorted_items else ([], [])
    
    ax.bar(keys, values)
    ax.set_xticklabels(keys, rotation=90, fontsize=4)
    ax.set_xlabel('Action')
    ax.set_ylabel('Proportion of all non-zero actions')
    fig.canvas.draw()

    return fig


def plot_to_array(fig):
    """
    Transform the matplotlib figure to a numpy array.
    """
    with io.BytesIO() as buff:
        fig.savefig(buff, format='raw')
        buff.seek(0)
        data = np.frombuffer(buff.getvalue(), dtype=np.uint8)
    w, h = fig.canvas.get_width_height()
    im = data.reshape((int(h), int(w), -1))
    im = im[:, :, :3]  # skip the alpha channel in rgba
    plt.close(fig)  # Close the figure to free memory
    return im