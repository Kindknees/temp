env_config_train: &env_config_train
  env_name: rte_case14_realistic_train
  keep_actions: ['change_bus']
  keep_observations : ['rho', 'gen_p', 'load_p', 'p_or', 'p_ex', 'timestep_overflow', 'topo_vect']
  medha_actions: True
  rho_threshold: 0.95
  run_until_threshold: True
  disable_line: -1

env_config_val: &env_config_val
  env_name: rte_case14_realistic_val
  keep_actions: ['change_bus']
  keep_observations : ['rho', 'gen_p', 'load_p', 'p_or', 'p_ex', 'timestep_overflow', 'topo_vect']
  medha_actions: True
  rho_threshold: 0.95
  run_until_threshold: True
  disable_line: -1

# 策略參數，Python 程式碼將直接讀取這裡的設定
choose_substation_agent_params: &choose_substation_agent_params

choose_action_agent_params: &choose_action_agent_params

# tune_config 現在只包含訓練和資源相關的超參數
tune_config:
  # 訓練超參數
  lr: 0.0005
  kl_coeff: 0.3
  lambda_: 0.95
  vf_loss_coeff: 0.9
  vf_clip_param: 1500
  num_epochs: 8      # PPOConfig 中對應到 num_sgd_iter
  entropy_coeff: 0.025
  clip_param: 0.5

  # Batching 參數
  rollout_fragment_length: 128
  train_batch_size: 1024
  sgd_minibatch_size: 256
  seed: 0
  
  # 資源設定
  num_workers: 2